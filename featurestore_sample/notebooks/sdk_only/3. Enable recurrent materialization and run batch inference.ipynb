{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Tutorial #3: Enable recurrent materialization and run batch inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "In this tutorial series you will experience how features seamlessly integrates all the phases of ML lifecycle: Prototyping features, training and operationalizing.\n",
    "\n",
    "In the previous part of the tutorial you learnt to experiment with features, train the model and register the model along with the feature-retrieval spec. In this tutorial you will learn how to run batch inference for the registered model.\n",
    "\n",
    "You will perform the following:\n",
    "- Enable recurrent materialization for the `transactions` feature set\n",
    "- Run batch inference pipeline on the registered model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Prerequisites\n",
    "1. Please ensure you have executed the previous parts of this tutorial series"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Configure Azure ML spark notebook\n",
    "\n",
    "1. In the \"Compute\" dropdown in the top nav, select \"Serverless Spark Compute\". \n",
    "1. Click on \"configure session\" in top status bar -> click on \"Python packages\" -> click on \"upload conda file\" -> select the file azureml-examples/sdk/python/featurestore-sample/project/env/conda.yml from your local machine; Also increase the session time out (idle time) if you want to avoid running the prerequisites frequently\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Start spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1745427971331
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "start-spark-session",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# run this cell to start the spark session (any code block will start the session ). This can take around 10 mins.\n",
    "print(\"start spark session\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Try running this if your the Conda yaml file takes too long to load in the spark configuration setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to install the required packages:\n",
    "# !pip install protobuf==3.19.6 aiohttp==3.8.4 azureml-featurestore[online]==1.1.1 azure-ai-ml==1.24.0 \\\n",
    "#     azure-mgmt-msi azure-mgmt-redis azure-mgmt-authorization==3.0.0 pandas==1.5.3 scikit-learn \\\n",
    "#     azureml-inference-server-http azure-cli"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Setup root directory for the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1745428029184
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "root-dir",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# please update the dir to ./Users/<your_user_alias> (or any custom directory you uploaded the samples to).\n",
    "# You can find the name from the directory structure in the left nav\n",
    "root_dir = \"Users/admin/Azure_MLOps_v3/featurestore_sample\"\n",
    "\n",
    "if os.path.isdir(root_dir):\n",
    "    print(\"The folder exists.\")\n",
    "else:\n",
    "    print(\"The folder does not exist. Please create or fix the path\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Initialize the project workspace CRUD client\n",
    "This is the current workspace where you will be running the tutorial notebook from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1745428074513
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "init-ws-crud-client",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "### Initialize the MLClient of this project workspace\n",
    "import os\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.identity import AzureMLOnBehalfOfCredential\n",
    "\n",
    "project_ws_sub_id = \"...\"\n",
    "project_ws_rg = \"...\"\n",
    "project_ws_name = \"...\"\n",
    "\n",
    "# connect to the project workspace\n",
    "ws_client = MLClient(\n",
    "    AzureMLOnBehalfOfCredential(), project_ws_sub_id, project_ws_rg, project_ws_name\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Initialize the feature store CRUD client\n",
    "Ensure you update the `featurestore_name` to reflect what you created in part 1 of this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1745428106343
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "init-fs-crud-client",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.identity import AzureMLOnBehalfOfCredential\n",
    "\n",
    "# feature store\n",
    "featurestore_name = (\n",
    "    \"...\"  # use the same name from part #1 of the tutorial\n",
    ")\n",
    "featurestore_subscription_id = \"...\"\n",
    "featurestore_resource_group_name = \"...\"\n",
    "\n",
    "# feature store ml client\n",
    "fs_client = MLClient(\n",
    "    AzureMLOnBehalfOfCredential(),\n",
    "    featurestore_subscription_id,\n",
    "    featurestore_resource_group_name,\n",
    "    featurestore_name,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Initialize the feature store core sdk client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1745428117214
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "init-fs-core-sdk",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# feature store client\n",
    "from azureml.featurestore import FeatureStoreClient\n",
    "from azure.ai.ml.identity import AzureMLOnBehalfOfCredential\n",
    "\n",
    "featurestore = FeatureStoreClient(\n",
    "    credential=AzureMLOnBehalfOfCredential(),\n",
    "    subscription_id=featurestore_subscription_id,\n",
    "    resource_group_name=featurestore_resource_group_name,\n",
    "    name=featurestore_name,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 1: Enable recurrent materialization on the `transactions` featureset\n",
    "\n",
    "In part 2 of this tutorial you enabled materialization and performed backfill on the transactions feature set. Backfill is an ondemand one-time operation to compute and store feature values in the materialization store. However when you want to perform inference of the model in production, you might want to keep the materilization store upto date by setting up recurrent materialization jobs. These jobs run on user defined schedule\n",
    "The recurrent job schedule works in the following way: \n",
    "- A window is defined by the interval and frequency. E.g., interval = 3 and frequency = Hour define a 3-hour window\n",
    "- The first window starts at the start_time defined in the RecurenceTrigger, and so on.\n",
    "- The first recurrent job will be submitted at the begining of the next window after the update time.\n",
    "- Later recurrent jobs will be submitted at every window after the first job.\n",
    "\n",
    "As explained in the previous parts of the tutorials, once data is materialized (backfill/recurrent materialization), feature retrieval will use the materialized data by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1745428173200
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "enable-recurrent-mat-txns-fset",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from azure.ai.ml.entities import RecurrenceTrigger\n",
    "\n",
    "transactions_fset_config = fs_client.feature_sets.get(name=\"transactions\", version=\"1\")\n",
    "\n",
    "# create a schedule that runs the materialization job every 3 hours\n",
    "transactions_fset_config.materialization_settings.schedule = RecurrenceTrigger(\n",
    "    interval=3, frequency=\"Hour\", start_time=datetime(2025, 4, 15, 0, 4, 10, 0)\n",
    ")\n",
    "\n",
    "fs_poller = fs_client.feature_sets.begin_create_or_update(transactions_fset_config)\n",
    "\n",
    "print(fs_poller.result())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### (Optional) Save the feature set asset  yaml with the updated settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1681791896733
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "dump-txn-fset-with-mat-yaml",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "## uncomment and run\n",
    "# transactions_fset_config.dump(root_dir + \"/featurestore/featuresets/transactions/featureset_asset_offline_enabled_with_schedule.yaml\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Track status of the recurrent materialization jobs in the feature store studio UI\n",
    "This job will every three hours. \n",
    "\n",
    "__Action__:\n",
    "\n",
    "1. Feel free to execute the next step for now (batch inference).\n",
    "1. In three hours check the recurrent job status via the UI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 2: Run the batch-inference pipeline\n",
    "\n",
    "In this step you will manually trigger the batch inference pipeline. In a production scenario, this could be trigerred by a ci/cd pipeline based on model registration/approval.\n",
    "\n",
    "The batch-inference has the following steps:\n",
    "\n",
    "1. Feature retrieval step: This use the same built-in feature retrieval component that we used in the training pipeline in the part 3 of the tutorial. Incase of training pipeline, we provided feature retreival spec as an input to the component. However in case of batch inference we will pass the registered model as the input and the component will look for feature retrieval spec in the model artifact. Another difference is that in case of training, the observation data had the target variable, however incase of batch inference it will not be present. The feature retrieval step will join the observation data with the features and output the data for batch inference.\n",
    "1. Batch inference: This step uses the batch inference input data from previous step, runs inference on the model and outputs the data by appending the predicted value.\n",
    "\n",
    "__Note:__ In this example we use a job for batch inference. You can also use Azure ML's batch endpoints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Please check batch_inference_pipeline.yaml and make sure the compute cluster is correct. Check these steps:\n",
    "- model_retrieval_step\n",
    "- inference_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1745429377624
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "run-batch-inf-pipeline",
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import load_job  # will be used later\n",
    "\n",
    "# set the batch inference  pipeline path\n",
    "batch_inference_pipeline_path = (\n",
    "    root_dir + \"/project/fraud_model/pipelines/batch_inference_pipeline.yaml\"\n",
    ")\n",
    "batch_inference_pipeline_definition = load_job(source=batch_inference_pipeline_path)\n",
    "\n",
    "# run the training pipeline\n",
    "batch_inference_pipeline_job = ws_client.jobs.create_or_update(\n",
    "    batch_inference_pipeline_definition\n",
    ")\n",
    "\n",
    "# stream the run logs\n",
    "ws_client.jobs.stream(batch_inference_pipeline_job.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Inspect the batch inference output data\n",
    "1. In the pipeline view, double click on `inference_step` -> in `outputs` card, copy the `Data` field. It will be something like `azureml_995abbc2-3171-461e-8214-c3c5d17ede83_output_data_data_with_prediction:1`. \n",
    "1. Paste it in the below cell with name and version separately (notice that the last character is the version, separated by a `:`).\n",
    "1. You will see the `predict_is_fraud` column generated by the batch inference pipeline\n",
    "\n",
    "Explanation: Since we did not provide a `name` and `version` in the `outputs` of the `inference_step` in the batch inference pipeline (`/project/fraud_mode/pipelines/batch_inference_pipeline.yaml`), the system created an untracked data asset with a guid as name and version as 1. In the next cell we will be getting the data path from the asset and displaying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1745429695926
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "inspect-batch-inf-output-data",
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "inf_data_output = ws_client.data.get(\n",
    "    name=\"...\",\n",
    "    version=\"1\",\n",
    ")\n",
    "inf_output_df = spark.read.parquet(inf_data_output.path)\n",
    "display(inf_output_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Tutorial \"5. Develop a feature set with custom source\" has instructions for deleting the resources."
   ]
  }
 ],
 "metadata": {
  "categories": [
   "SDK v2",
   "sdk",
   "python",
   "featurestore_sample",
   "notebooks",
   "sdk_only"
  ],
  "celltoolbar": "Edit Metadata",
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "codemirror_mode": "ipython",
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython",
   "version": "3.8.0"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "synapse_widget": {
   "state": {
    "fe048257-d9d0-498a-9820-fb1d7f474bd5": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "sum",
        "categoryFieldKeys": [
         "0"
        ],
        "chartType": "bar",
        "isStacked": false,
        "seriesFieldKeys": [
         "0"
        ]
       },
       "tableOptions": {},
       "type": "details"
      }
     },
     "sync_state": {
      "isSummary": false,
      "language": "scala",
      "table": {
       "rows": [
        {
         "0": "261.45",
         "1": "5.0",
         "2": "0.0",
         "3": "0.0",
         "4": "0.0",
         "5": "366.0",
         "6": "0.0",
         "7": "0.0",
         "8": "0.0",
         "9": "0.0",
         "10": "1",
         "index": 1
        },
        {
         "0": "261.45",
         "1": "5.0",
         "2": "260.370211",
         "3": "1.0",
         "4": "0.0",
         "5": "366.0",
         "6": "0.0",
         "7": "0.0",
         "8": "0.0",
         "9": "0.0",
         "10": "0",
         "index": 2
        },
        {
         "0": "1598.0",
         "1": "8.0",
         "2": "0.0",
         "3": "0.0",
         "4": "0.0",
         "5": "366.0",
         "6": "0.0",
         "7": "0.0",
         "8": "0.0",
         "9": "0.0",
         "10": "1",
         "index": 3
        },
        {
         "0": "1598.0",
         "1": "8.0",
         "2": "2516.6902",
         "3": "0.0",
         "4": "2.0",
         "5": "366.0",
         "6": "0.0",
         "7": "0.0",
         "8": "0.0",
         "9": "0.0",
         "10": "0",
         "index": 4
        },
        {
         "0": "531.24",
         "1": "15.0",
         "2": "0.0",
         "3": "0.0",
         "4": "0.0",
         "5": "797.0",
         "6": "0.0",
         "7": "0.0",
         "8": "0.0",
         "9": "0.0",
         "10": "1",
         "index": 5
        }
       ],
       "schema": [
        {
         "key": "0",
         "name": "transactionAmount",
         "type": "double"
        },
        {
         "key": "1",
         "name": "localHour",
         "type": "double"
        },
        {
         "key": "2",
         "name": "transactionAmountUSD",
         "type": "double"
        },
        {
         "key": "3",
         "name": "digitalItemCount",
         "type": "double"
        },
        {
         "key": "4",
         "name": "physicalItemCount",
         "type": "double"
        },
        {
         "key": "5",
         "name": "accountAge",
         "type": "double"
        },
        {
         "key": "6",
         "name": "transaction_amount_7d_sum",
         "type": "double"
        },
        {
         "key": "7",
         "name": "transaction_amount_3d_sum",
         "type": "double"
        },
        {
         "key": "8",
         "name": "numPaymentRejects1dPerUser",
         "type": "double"
        },
        {
         "key": "9",
         "name": "transaction_amount_7d_avg",
         "type": "double"
        },
        {
         "key": "10",
         "name": "predict_is_fraud",
         "type": "bigint"
        }
       ],
       "truncated": false
      }
     },
     "type": "Synapse.DataFrame"
    }
   },
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
